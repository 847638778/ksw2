\documentclass{bioinfo}
\copyrightyear{2017}
\pubyear{2017}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\rmfamily{\it #1}}
\SetCommentSty{mycommfont}
\SetKwComment{Comment}{$\triangleright$\ }{}

\usepackage{natbib}
\bibliographystyle{apalike}

\begin{document}
\firstpage{1}

\title[Alignment with DP]{On the implementation of pairwise alignment with dynamic programming}
\author[Li]{Heng Li}
\address{Broad Institute, 415 Main Street, Cambridge, MA 02142, USA}

\maketitle

\begin{abstract}
This \emph{informal} note disambiguates a few different formulations of
pairwise alignment with dynamic programming (DP), investigates the choice of
gap cost function and discusses the implementation of DP-based alignment with a
focus on practical applications. It targets developers who want to grasp
DP-based alignment and implement it to high performance. Importantly, this note
is not a general tutorial. Readers are required to understand the basis of
pairwise alignment before going through the note.
\end{abstract}

\section{Early literature}

\citet{Needleman:1970aa} first proposed to align two biological sequences with
dynamic programming (DP). This is a global alignment algorithm in the sense
that each residue in the pair of sequences is required to be aligned.
\citet{Smith:1981aa} adapted the algorithm to find local hits. They used a
generic \emph{gap cost function} $\gamma(k)$. For two sequences of length $m$
and $n$, respectively, the time complexity to find the optima is
$O(mn\cdot\max\{m,n\})$. \citet{Gotoh:1982aa} showed that when the cost
function takes a form $\gamma_1(k)=q+k\cdot e$, which is called \emph{affine gap
cost}, it is possible to solve the alignment problem in $O(mn)$ time.
\citet{Altschul:1986aa} fixed an issue in the original Gotoh's algorithm and
introduced the formulation we commonly use today. On more general
gap cost, \citet{Miller:1988aa} devised an $O(mn\log\max\{m,n\})$ algorithm
if $\gamma(k)$ is a concave function. For a piece-wise affine cost composed of
$p$ affine cost functions for different gap lengths, their algorithm could find
the optimal alignment in $O(mn\log p)$ time. \citet{Gotoh:1990aa} proposed an
$O(mn\cdot p)$ algorithm, which is simpler and probably faster for small $p$ in
practice. As the title of Gotoh's paper suggests, piece-wise cost helps
to find long gaps in the alignment.

\section{Alignment with dynamic programming}

In this section, we assume there are two sequences. One is the \emph{target}
sequence or \emph{reference} sequence of length $n$; the other sequence is the
\emph{query} sequence of length $m$. Gaps on the target sequence are
\emph{deletions}; gaps on the query sequence are \emph{insertions}. Function
$s(i,j)$, $0\le i<n$ and $0\le j<m$, gives the score between the $i$-th residue
on the target and the $j$-th residue on the query.

We will focus on global alignment algorithms because it is usually more complex
than local alignment due to non-trivial initial conditions. We will also
briefly touch other types of alignment (e.g. glocal alignment).

\subsection{Linear gap cost}

A linear gap cost function is $\gamma_0(k;e)=k\cdot e$. Let the optimal score up
to cell $(i,j)$ is $H_{ij}$. The equation to compute $H_{n-1,m-1}$ is:
\begin{equation}\label{eq:linear}
H_{ij}=\left\{\begin{array}{l}
H_{i-1,j-1}+s(i,j) \\
H_{i-1,j}-e \\
H_{i,j-1}-e
\end{array}\right.
\end{equation}

\subsection{Edit distance}

\emph{Edit distance} is the minimum sum of substitutions, insertions and
deletions among all possible alignments between two sequences. It is also
called \emph{Levenshtein distance}~\citep{Levenshtein:1966aa}, though the
author did not provide an algorithm to compute the distance.

Edit distance can be computed with Eq.~(\ref{eq:linear}) if we set matching
score to 0, mismatch score to -1 and $e=1$. Faster algorithms exist.
\citet{Landau:1986aa} found an $O(kn)$ algorithm that guarantees to find
the optimal solution if the edit distance is no larger than $k$. It is
particularly fast if the edit distance is small~\citep{Sosic:2015aa}.

\citet{Myers:1999aa} made an important observation that the difference between
adjacent cells
\[\left\{\begin{array}{l}
u_{ij}\triangleq H_{ij}-H_{i-1,j}\\
v_{ij}\triangleq H_{ij}-H_{i,j-1}
\end{array}\right.\]
only take values $-1$, $0$ or $1$ and can be computed in the diagonal direction
in the DP matrix. This enables bit-level parallelization. Combined with
bounding~\citep{Ukkonen:1985aa}, Myers' algorithm is significantly faster than
the standard algorithm for long sequences. Edlib~\citep{Sosic:2017aa} provides
an efficient implementation of this algorithm with added functionality.
\citet{Loving:2014aa} extended the bit-parallelism to more general integer
scoring. The Suzuki's formulation below was also inspired by
\citet{Myers:1999aa}.

\subsection{Affine gap cost: Durbin's formulation}

With a linear cost function, a long gap is effectively considered to arise from
a series of small gaps. In evolution, however, a long gap at times results from
one event (e.g. a transposon insertion). A linear gap cost often breaks a long
gap into small pieces and complicates the interpretation of alignment.
Therefore, it is discouraged to use a linear cost to produce alignment. The
most common solution to this issue is to use an affine gap cost
$\gamma_1(k;q,e)=q+k\cdot e$. There are a few different ways to do pairwise
alignment under an affine gap cost.

To give alignment a probablitistic interpretation, \citep{Durbin:1998uq} 
introduced
\begin{equation}\label{eq:durbin}
\left\{\begin{array}{l}
M_{ij}=\max\{M_{i-1,j-1}, E_{i-1,j-1}, F_{i-1,j-1}\} + s(i,j)\\
E_{ij}=\max\{M_{i-1,j}-q, E_{i-1,j}\} - e\\
F_{ij}=\max\{M_{i,j-1}-q, F_{i,j-1}\} - e
\end{array}\right.
\end{equation}
This formulation has a natural connection to pair-HMM with each state having a
clear meaning in alignment. It, however, has one problem: it disallows
transitions between $E$ and $F$ states and thus forbids insertions immediately
followed by deletions (and vice versa). When the gap extension cost $e$ is
smaller than a mismatch cost, transitions between $E$ and $F$ may yield a
better alignment score. It is possible to add transitions between $E$ and $F$
in Eq.~(\ref{eq:durbin}), but in practice, AE86's
formulation~\citep{Altschul:1986aa} will be faster to implement.

\subsection{Affine gap cost: AE86's formulation}

In Eq.~(\ref{eq:durbin}), if we let:
\[H_{ij}=\max\{M_{ij},E_{ij},F_{ij}\}\]
Durbin's formulation allowing $E$--$F$ transitions becomes
\begin{equation*}
\left\{\begin{array}{l}
E_{ij}=\max\{H_{i-1,j}-q, E_{i-1,j}\} - e \\
F_{ij}=\max\{H_{i,j-1}-q, F_{i,j-1}\} - e \\
H_{ij}=\max\{H_{i-1,j-1}+S(i,j), E_{ij}, F_{ij}\}
\end{array}\right.
\end{equation*}
This is AE86's formulation. In practice, we also often compute the cells in the
following order
\begin{equation}\label{eq:ae86}
\left\{\begin{array}{l}
H_{ij}=\max\{H_{i-1,j-1}+S(i,j), E_{ij}, F_{ij}\}\\
E_{i+1,j}=\max\{H_{ij}-q, E_{ij}\} - e \\
F_{i,j+1}=\max\{H_{ij}-q, F_{ij}\} - e
\end{array}\right.
\end{equation}
with initial conditions
\begin{equation}
\left\{\begin{array}{ll}
H_{-1,-1}=0\\
H_{-1,j}=-q-e-j\cdot e & (0\le j<m)\\
H_{i,-1}=-q-e-i\cdot e & (0\le i<n)\\
E_{0j}=-2q-2e-j\cdot e & (0\le j<m)\\
F_{i0}=-2q-2e-i\cdot e & (0\le i<n)
\end{array}\right.
\end{equation}
We don't need $E_{-1,\cdot}$ or $F_{\cdot,-1}$ because Eq.~(\ref{eq:ae86})
does not start with these initial values. Algorithm~\ref{algo:ae86} gives
the details of AE86. At the beginning of each iteration at line 1, $f=F_{ij}$,
$h=H_{i,j-1}$, $H[j]=H_{i-1,j-1}$ and $E[j]=E_{ij}$.

\begin{algorithm}[tb]
\DontPrintSemicolon
\footnotesize
\KwIn{Targe sequence $T$ and query $Q$; scoring matrix $S(\cdot,\cdot)$ and
affine gap cost $\gamma_1(k;q,e)=q+k\cdot e$}
\KwOut{Best alignment score between $T$ and $Q$}
\BlankLine
\textbf{Function} {\sc AlignScore}$(T,Q,q,e)$
\Begin {
	\For (\Comment*[f]{Generate query profile}) {$a\in\Sigma$} {
		\For{$j\gets0$ \KwTo $|Q|-1$} {
			$P[a][j]\gets S(Q[j],a)$\;
		}
	}
	\For{$j\gets0$ \KwTo $|Q|-1$} {
		$H[j]\gets-q-j\cdot e$\Comment*[r]{$H[j]=H_{-1,j-1}$}
		$E[j]\gets-2q-2e-j\cdot e$\Comment*[r]{$E[j]=E_{0j}$}
	}
	$H[j]\gets 0$\Comment*[r]{$H_{-1,-1}=0$}
	\For{$i\gets0$ \KwTo $|T|-1$} {
		$f\gets-2q-2e-i\cdot e$\Comment*[r]{$f=F_{i0}$}
		$h\gets-q-e-i\cdot e$\Comment*[r]{$h=H_{i,-1}$}
		$p\gets P[T[i]]$\;
		\For{$j\gets0$ \KwTo $|Q|-1$} {
			\nl$s\gets p[j]$\Comment*[r]{$s=S(Q[j],T[i])$}
			$h'\gets\max\{H[j]+s,E[j],f\}$\;
			$H[j]\gets h$\Comment*[r]{$H[j]=H_{i,j-1}$}
			$h\gets h'$\Comment*[r]{$h=H_{ij}$}
			$E[j]\gets\max\{h-q,E[j]\}-e$\Comment*[r]{$E[j]=E_{i+1,j}$}
			$f\gets\max\{h-q,f\}-e$\Comment*[r]{$f=F_{i,j+1}$}
		}
		$H[|Q|]\gets h$\;
	}
	\Return $H[|Q|]$\;
}
\caption{AE86's formulation with affine gap cost}\label{algo:ae86}
\end{algorithm}

\subsection{Affine gap cost: SIMD acceleration}

SIMD CPU instructions perform one action on a vector of data at the same time.
For example, with SSE2, a type of SIMD, we can compute the sum of two vectors
of sixteen 8-bit integers with one CPU instruction. This is usually times
faster than summing with a loop. How many data can be processed with one SIMD
instruction depends on the number of bits in the vector and the max value
of each element in the vector. For example, SSE instructions operate on 128-bit
vectors. We can process four 32-bit integers or eight 16-bit integers at the
same time. AVX instructions operate on 256-bit vectors. It doubles the
bandwidth of SSE.

SIMD has been used to speed up DP-based pairwise alignment.  There are two
general classes of SIMD algorithms: inter-sequence and intra-sequence.
Inter-sequence algorithms~\citep{Rognes:2011aa} align multiple pairs of
sequences at the same time. It is conceptually easier to implement but tricky
to use with other alignment routines. Intra-sequence algorithms align one
sequence at a time.  There are several ways to implement intra-sequence
pairwise alignment, depending on how to organize multiple data into one vector.
For simplicity, we assume each vector consists of four elements.

\citet{Wozniak:1997aa} put $(H_{ij},H_{i+1,j-1},H_{i+2,j-2},H_{i+3,j-3})$
into a vector and fills the DP matrix along its diagonal.
\citet{Rognes:2000aa} took a block of row cells into a vector
$(H_{ij},H_{i,j+1},H_{i,j+2},H_{i,j+3})$. \citet{Farrar:2007hs} interleaved rows
into $(H_{ij},H_{i,t+j},H_{i,2t+j},H_{i,3t+j})$ in a striped manner, where
$t=\lfloor(m+3)/4\rfloor$ if a vector consists of 4 elements. In practice,
Farrar's striped algorithm is the fastest and most often
used~\citep{Szalkowski:2008aa,Zhao:2013aa}. \citet{Daily:2016aa} developed a
programming library that implements all three intra-sequence algorithms.

\subsection{Affine gap cost: Suzuki's formulation}

When working with long sequences that yield large alignment scores,
we may need to use 32-bit integers to hold the score arrays. With SSE, we can
only process four cells at a time. Inspired by \citet{Myers:1999aa} and
\citet{Loving:2014aa}, \href{https://github.com/ocxtal}{Hajime Suzuki} proposed
to rewrite Eq.~(\ref{eq:ae86}) with differences between cells:
\begin{equation}
\left\{\begin{array}{l}
u_{ij}\triangleq H_{ij}-H_{i-1,j}\\
v_{ij}\triangleq H_{ij}-H_{i,j-1}\\
x_{ij}\triangleq E_{i+1,j}-H_{ij}\\
y_{ij}\triangleq F_{i,j+1}-H_{ij}
\end{array}\right.
\end{equation}
as
\begin{equation}\label{eq:suzuki}
\left\{\begin{array}{l}
z_{ij}=\max\{s(i,j),x_{i-1,j}+v_{i-1,j},y_{i,j-1}+u_{i,j-1}\}\\
u_{ij}=z_{ij}-v_{i-1,j}\\
v_{ij}=z_{ij}-u_{i,j-1}\\
x_{ij}=\max\{0,x_{i-1,j}+v_{i-1,j}-z_{ij}+q\}-q-e\\
y_{ij}=\max\{0,y_{i,j-1}+u_{i,j-1}-z_{ij}+q\}-q-e
\end{array}\right.
\end{equation}
where $z'_{ij}$ is a temporary variable that does not need to be stored.
We can prove that all variables in these equations are bounded by gap costs and
the extreme match and mismatch scores, but not by the sequence lengths or the
peak alignment score. For small scores, we can encode 16 cells in one SSE
vector. In practice, SSE vectorization of Suzuki's formulation yields better
performance for long sequences.

\subsection{Piece-wise affine gap cost}

Affine gap cost still does not solve the long gap issue. Suppose the affine gap
cost function is $\gamma_1(k)=q+k\cdot e$. Let $a>0$ be the max match score and
$b>0$ the max mismatch cost. Given a delete of length $l$, if $\lceil
q/(a+b)\rceil$ or more residues on the query adjacent to the gap are mismatches
but have a perfect match to a subsequence in the gap, the perfect match will
yield a higher alignment score and split the long gap in two.  For a large $l$
and noisy reads, it is not infrequent that a long gap to be split by errors. We
would prefer to increase the gap open cost $q$ to avoid such a split, but this
would contradict the high INDEL error rate of some sequencing data.

The root cause of this dilemma is that gaps are caused by two different
mechanisms: evolution which may create a long gap with one event, and
sequencing errors which generate gaps as relatively independent events. A
better gap cost is a concave function such that $\gamma(k+1)-\gamma(k)$ is
smaller with larger $k$.

A simple concave gap cost is
\[
\gamma_2(k;q,e,\tilde{q},\tilde{e})=\min\{q+k\cdot e,\tilde{q}+k\cdot\tilde{e}\}
\]
on the condition that $q+e<\tilde{q}+\tilde{e}$ and $e>\tilde{e}$. Effectively,
this cost function applies $\gamma_1(k;q,e)$ to gaps shorter than
$\lceil(\tilde{q}-q)/(e-\tilde{e})\rceil$ and applies
$\gamma_1(k;\tilde{q},\tilde{e})$ to longer gaps. We can compute the maximal
alignment score under $\gamma_2(k)$ with
\begin{equation}\label{eq:affine2}
\left\{\begin{array}{l}
H_{ij} = \max\{H_{i-1,j-1}+s(i,j),E_{ij},F_{ij},\tilde{E}_{ij},\tilde{F}_{ij}\}\\
E_{i+1,j}= \max\{H_{ij}-q,E_{ij}\}-e\\
F_{i,j+1}= \max\{H_{ij}-q,F_{ij}\}-e\\
\tilde{E}_{i+1,j}= \max\{H_{ij}-\tilde{q},\tilde{E}_{ij}\}-\tilde{e}\\
\tilde{F}_{i,j+1}= \max\{H_{ij}-\tilde{q},\tilde{F}_{ij}\}-\tilde{e}
\end{array}\right.
\end{equation}
We may generalize Eq.~(\ref{eq:affine2}) to more pieces~\citep{Gotoh:1990aa} at the
cost of performance. Eq.~(\ref{eq:suzuki}) can be extended to work with
piece-wise affine cost in a similar manner.

\bibliography{aln-dp}
\end{document}
